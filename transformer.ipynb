{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfb0GTzm9r3f"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        " \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Erq8-ciWCrs"
      },
      "source": [
        "#모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBeo6-zcWGoO"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_emb, trg_emb, fc_layer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_emb = src_emb\n",
        "        self.trg_emb = trg_emb\n",
        "        self.fc_layer = fc_layer\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor, input_mask, target_mask):\n",
        "        context = self.encode(input_tensor, input_mask)\n",
        "        out = self.decode(target_tensor, target_mask, context, input_mask)\n",
        "        out = self.fc_layer(out)\n",
        "        out = F.log_softmax(out, dim=-1)\n",
        "        out=out.squeeze()\n",
        "        return out\n",
        "\n",
        "    def encode(self, x, mask):\n",
        "        out = self.encoder(self.src_emb(x), mask)\n",
        "        return out\n",
        "    \n",
        "    def decode(self, x, mask, encoder_output, encoder_mask):\n",
        "        out = self.decoder(self.trg_emb(x), mask, encoder_output, encoder_mask)\n",
        "        return out"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YISiz5w5WK2K"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, layers):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for l in range(n_layers):\n",
        "            self.layers.append(copy.deepcopy(layers))\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "        return out"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTiJ42V8WcVy"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attn_layer, ff_layer, norm):\n",
        "        super().__init__()\n",
        "        self.attn_layer = attn_layer\n",
        "        self.ff_layer = ff_layer\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = self.attn_layer(x, x, x, mask)\n",
        "        out = x + out\n",
        "        out = self.norm(out)\n",
        "        \n",
        "        out_ = self.ff_layer(out)\n",
        "        out = out + out_\n",
        "        out = self.norm(out)\n",
        "        return out"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QhXpCPWgjM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, layers):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for l in range(n_layers):\n",
        "            self.layers.append(copy.deepcopy(layers))\n",
        "\n",
        "    def forward(self, x, mask, encoder_output, encoder_mask):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask, encoder_output, encoder_mask)\n",
        "        return out"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HiMLxclWjc8"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, masked_attn_layer, attn_layer, ff_layer, norm):\n",
        "        super().__init__()\n",
        "        self.masked_attn_layer = masked_attn_layer\n",
        "        self.attn_layer = attn_layer\n",
        "        self.ff_layer = ff_layer\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, x, mask, encoder_output, encoder_mask):\n",
        "        out = self.masked_attn_layer(x, x, x, mask)\n",
        "        out = x + out\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out_ = self.attn_layer(out, encoder_output, encoder_output, encoder_mask)\n",
        "        out = out + out_\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out_ = self.ff_layer(out)\n",
        "        out = out + out_\n",
        "        out = self.norm(out)\n",
        "        return out"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewZFAyFAWw7z"
      },
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, d_embed, dropout_p, max_len = 4000):\n",
        "        super().__init__()\n",
        "        encoding = torch.zeros(max_len, d_embed)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0)/ d_embed))\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        encoding = encoding.unsqueeze(0)\n",
        "        self.encoding = encoding\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + Variable(self.encoding[:, :x.size(1)], requires_grad=False).to(device)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6tooyI1W2vb"
      },
      "source": [
        "class MultiHeadAttn(nn.Module):\n",
        "    def __init__(self, d_model, n_head, qkv_fc_layer, fc_layer, dropout_p):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.q_fc = copy.deepcopy(qkv_fc_layer)\n",
        "        self.k_fc = copy.deepcopy(qkv_fc_layer)\n",
        "        self.v_fc = copy.deepcopy(qkv_fc_layer)\n",
        "        self.fc_layer = fc_layer\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def calcAttn(self, q, k, v, mask, dropout=None):\n",
        "        dk = k.size(-1)\n",
        "        score = torch.matmul(q, k.transpose(-2, -1))\n",
        "        score = score / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask==0, -1e9)\n",
        "        out = F.softmax(score, dim=-1)\n",
        "        if dropout is not None:\n",
        "            out = dropout(out)\n",
        "        out = torch.matmul(out, v)\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        n_batch = q.shape[0]\n",
        "        \n",
        "        def transform(x, fc_layer):\n",
        "            out = fc_layer(x)\n",
        "            out = out.view(n_batch, -1, self.n_head, self.d_model//self.n_head)\n",
        "            out = out.transpose(1, 2)\n",
        "            return out\n",
        "        \n",
        "        q = transform(q, self.q_fc)\n",
        "        k = transform(k, self.k_fc)\n",
        "        v = transform(v, self.v_fc)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        out = self.calcAttn(q, k, v, mask, self.dropout)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.contiguous().view(n_batch, -1, self.d_model)\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtgA5aVcLPv"
      },
      "source": [
        "class Feedforward(nn.Module):\n",
        "    def __init__(self, first, second, dropout_p):\n",
        "        super().__init__()\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.first(x)\n",
        "        out = F.relu(x)\n",
        "        out = self.dropout(out)\n",
        "        out = self.second(out)\n",
        "        return out"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-y5SUDRuy-B"
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_embed, vocab):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), d_embed)\n",
        "        self.vocab = vocab\n",
        "        self.d_embed = d_embed\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x) * math.sqrt(self.d_embed)\n",
        "        return out"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTubw1avu0_5"
      },
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, embedding, positional_encoding):\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.embedding = nn.Sequential(embedding, positional_encoding)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        return out"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39TCiBiUu5Ix"
      },
      "source": [
        "\n",
        "def make_model(\n",
        "    src_vocab, \n",
        "    trg_vocab, \n",
        "    d_embed = 512, \n",
        "    n_layer = 6, \n",
        "    d_model = 512, \n",
        "    n_head = 8, \n",
        "    d_ff = 512,\n",
        "    dropout_p = 0.1):\n",
        "\n",
        "    cp = lambda x : copy.deepcopy(x).to(device)\n",
        "\n",
        "    multi_head_attention_layer = MultiHeadAttn(\n",
        "                                    d_model = d_model,\n",
        "                                    n_head = n_head,\n",
        "                                    qkv_fc_layer = nn.Linear(d_embed, d_model),\n",
        "                                    fc_layer = nn.Linear(d_model, d_embed),\n",
        "                                    dropout_p = dropout_p)\n",
        "    \n",
        "    position_wise_feed_forward_layer = Feedforward(\n",
        "                                         first = nn.Linear(d_embed, d_ff),\n",
        "                                         second = nn.Linear(d_ff, d_embed),\n",
        "                                         dropout_p = dropout_p)\n",
        "    \n",
        "    norm_layer = nn.LayerNorm(d_embed, eps=1e-6)\n",
        "\n",
        "    model = Transformer(\n",
        "                src_emb = TransformerEmbedding(\n",
        "                                embedding = Embedding(\n",
        "                                                d_embed = d_embed, \n",
        "                                                vocab = src_vocab).to(device), \n",
        "                                positional_encoding = PositionEncoding(\n",
        "                                                d_embed = d_embed,\n",
        "                                                dropout_p = dropout_p).to(device)), \n",
        "                trg_emb = TransformerEmbedding(\n",
        "                                embedding = Embedding(\n",
        "                                                d_embed = d_embed, \n",
        "                                                vocab = trg_vocab).to(device), \n",
        "                                positional_encoding = PositionEncoding(\n",
        "                                                d_embed = d_embed,\n",
        "                                                dropout_p = dropout_p).to(device)),\n",
        "                encoder = Encoder(\n",
        "                                layers = EncoderLayer(\n",
        "                                                attn_layer = cp(multi_head_attention_layer),\n",
        "                                                ff_layer = cp(position_wise_feed_forward_layer),\n",
        "                                                norm = cp(norm_layer)),\n",
        "                                n_layers = n_layer),\n",
        "                decoder = Decoder(\n",
        "                                layers = DecoderLayer(\n",
        "                                                masked_attn_layer = cp(multi_head_attention_layer),\n",
        "                                                attn_layer = cp(multi_head_attention_layer),\n",
        "                                                ff_layer = cp(position_wise_feed_forward_layer),\n",
        "                                                norm = cp(norm_layer)),\n",
        "                                n_layers = n_layer),\n",
        "                fc_layer = nn.Linear(d_model, len(trg_vocab)).to(device))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSZv5f8xVDiu"
      },
      "source": [
        "# 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6OFOaSB-Lri"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    \n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwdNOk3k-9e5"
      },
      "source": [
        "tr_e = open('/content/drive/MyDrive/ColabFiles/wmt16/train.en').read().strip().split('\\n')\n",
        "tr_d = open('/content/drive/MyDrive/ColabFiles/wmt16/train.de').read().strip().split('\\n')\n",
        "te_e = open('/content/drive/MyDrive/ColabFiles/wmt16/test.en').read().strip().split('\\n')\n",
        "te_d = open('/content/drive/MyDrive/ColabFiles/wmt16/test.de').read().strip().split('\\n')\n",
        "va_e = open('/content/drive/MyDrive/ColabFiles/wmt16/val.en').read().strip().split('\\n')\n",
        "va_d = open('/content/drive/MyDrive/ColabFiles/wmt16/val.de').read().strip().split('\\n')\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm5kZ0j9De0F"
      },
      "source": [
        "tr_e = [normalizeString(s) for s in tr_e]\n",
        "tr_d = [normalizeString(s) for s in tr_d]\n",
        "te_e = [normalizeString(s) for s in te_e]\n",
        "te_d = [normalizeString(s) for s in te_d]\n",
        "va_e = [normalizeString(s) for s in va_e]\n",
        "va_d = [normalizeString(s) for s in va_d]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1npO8x1KkJbS"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3  # SOS 와 EOS PAD 포함\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWUsgqfv1d-x"
      },
      "source": [
        "MAX_LENGTH = 0\n",
        "\n",
        "for sen in tr_e:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())\n",
        "\n",
        "for sen in tr_d:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())\n",
        "\n",
        "for sen in te_e:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())\n",
        "\n",
        "for sen in te_d:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())\n",
        "\n",
        "for sen in va_e:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())\n",
        "\n",
        "for sen in va_d:\n",
        "    if len(sen.split()) > MAX_LENGTH:\n",
        "        MAX_LENGTH = len(sen.split())"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2OVAq8EkNmi"
      },
      "source": [
        "lang_e = Lang('e')\n",
        "lang_d = Lang('d')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGd6dr5eEec"
      },
      "source": [
        "pair_tr=[]\n",
        "for i in range(len(tr_e)):\n",
        "    pair_tr.append((tr_d[i], tr_e[i]))\n",
        "\n",
        "pair_te=[]\n",
        "for i in range(len(te_e)):\n",
        "    pair_te.append((te_d[i], te_e[i]))\n",
        "\n",
        "pair_va=[]\n",
        "for i in range(len(va_e)):\n",
        "    pair_va.append((va_d[i], va_e[i]))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au_tTdKikqqD"
      },
      "source": [
        "for pair in pair_tr:\n",
        "    lang_d.addSentence(pair[0])\n",
        "    lang_e.addSentence(pair[1])\n",
        "\n",
        "for pair in pair_te:\n",
        "    lang_d.addSentence(pair[0])\n",
        "    lang_e.addSentence(pair[1])\n",
        "\n",
        "for pair in pair_va:\n",
        "    lang_d.addSentence(pair[0])\n",
        "    lang_e.addSentence(pair[1])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Nj2RayrM1c",
        "outputId": "59ace7a5-81c5-43a9-855d-652f07051b8e"
      },
      "source": [
        "print(lang_e.n_words)\n",
        "print(lang_d.n_words)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9980\n",
            "18295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMmRLLYQe_vT"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(lang_d, pair[0])\n",
        "    target_tensor = tensorFromSentence(lang_e, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOhVMTJYlrwX"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S59eEE-n1eg"
      },
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, trg=None, pad=3):\n",
        "        self.src = src.T\n",
        "        self.src_mask = (self.src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg.T[:, :-1]\n",
        "            self.trg_y = trg.T[:,1:]\n",
        "            self.trg_mask = self.make_mask(self.trg, pad)\n",
        "            self.lengths = torch.sum((self.trg_y != pad),dim=1)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def subsequent_mask(self, size):\n",
        "        attn_shape = (1, size, size)\n",
        "        mask = np.triu(np.ones(attn_shape), k = 1).astype('uint8')\n",
        "        return torch.from_numpy(mask) == 0\n",
        "\n",
        "    def make_mask(self, trg, pad):\n",
        "        trg_mask = (trg != pad).unsqueeze(-2)\n",
        "        trg_mask = trg_mask & Variable(self.subsequent_mask(trg.size(-1)).type_as(trg_mask.data))\n",
        "        return trg_mask"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKe7-IQL_fbo"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkmhwT3E8AWp"
      },
      "source": [
        "def trainIters(model, \n",
        "               criterion,\n",
        "               optimizer,\n",
        "               n_iters,\n",
        "               print_every=1000,\n",
        "               plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_loss = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    training_pairs = [tensorsFromPair(random.choice(pair_tr)) \n",
        "    for iter in range(1, n_iters+1)]\n",
        "    for iter in range(1, n_iters+1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        batch = Batch(input_tensor, target_tensor)\n",
        "\n",
        "        batch.src = batch.src.to(device)\n",
        "        batch.trg = batch.trg.to(device)\n",
        "        batch.src_mask = batch.src_mask.to(device)\n",
        "        batch.trg_mask = batch.trg_mask.to(device)\n",
        "\n",
        "        loss = train(batch.src, batch.trg, batch.src_mask, batch.trg_mask, model, optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_loss.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    return plot_loss"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk4aLJII_ZWA"
      },
      "source": [
        "def train(input_tensor, target_tensor, input_mask, target_mask, model, optimizer, criterion):\n",
        "    loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_tensor, target_tensor, input_mask, target_mask)\n",
        "    loss = criterion(outputs, target_tensor.contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()/tgt_len"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dQWFQ8DvMZl",
        "outputId": "c6bcdcee-1a14-4bf4-f14e-791e044aee9e"
      },
      "source": [
        "MAX_LENGTH = 32\n",
        "\n",
        "src_vocab_size = lang_d.n_words\n",
        "tgt_vocab_size = lang_e.n_words\n",
        "\n",
        "src_len = MAX_LENGTH\n",
        "tgt_len = MAX_LENGTH+1\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "d_k = d_v = 64\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "\n",
        "model = make_model(lang_d.index2word.values(), lang_e.index2word.values())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "losslist = trainIters(model, criterion, optimizer, n_iters=10000)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 34s (- 5m 9s) (1000 10%) 0.1025\n",
            "1m 6s (- 4m 27s) (2000 20%) 0.0450\n",
            "1m 40s (- 3m 54s) (3000 30%) 0.0334\n",
            "2m 13s (- 3m 19s) (4000 40%) 0.0249\n",
            "2m 47s (- 2m 47s) (5000 50%) 0.0215\n",
            "3m 19s (- 2m 13s) (6000 60%) 0.0174\n",
            "3m 53s (- 1m 40s) (7000 70%) 0.0168\n",
            "4m 25s (- 1m 6s) (8000 80%) 0.0157\n",
            "4m 59s (- 0m 33s) (9000 90%) 0.0113\n",
            "5m 33s (- 0m 0s) (10000 100%) 0.0126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "BbFDKgwTVdeP",
        "outputId": "e3a37c63-d5f3-483e-80d6-3e1f83bdf35f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losslist)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3c22a64a50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5LyITgswcrH"
      },
      "source": [
        "#TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5DJSR_dNjLh"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vGOhTo2UTKB"
      },
      "source": [
        "def embToWord(sentence, lang):\n",
        "    wordlist=[]\n",
        "    for word in sentence:\n",
        "        wordlist.append(\n",
        "            lang.index2word[torch.argmax(word).item()]\n",
        "            )\n",
        "    return wordlist"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYgIaQbGNj7I"
      },
      "source": [
        "def test(model):\n",
        "    testcorpus = []\n",
        "    test_pairs = []\n",
        "    targetcorpus = []\n",
        "\n",
        "    for i in range(0, len(pair_te)):\n",
        "        test_pairs.append(tensorsFromPair(pair_te[i]))\n",
        "    for iter in range(0, len(pair_te)):\n",
        "        test_pair = test_pairs[iter]\n",
        "        input_tensor = test_pair[0]\n",
        "        target_tensor = test_pair[1]\n",
        "\n",
        "        batch = Batch(input_tensor, target_tensor)\n",
        "\n",
        "        batch.src = batch.src.to(device)\n",
        "        batch.trg = batch.trg.to(device)\n",
        "        batch.src_mask = batch.src_mask.to(device)\n",
        "        batch.trg_mask = batch.trg_mask.to(device)\n",
        "        testcorpus.append(\n",
        "            embToWord(model(batch.src, batch.trg, batch.src_mask, batch.trg_mask), lang_e)\n",
        "            )\n",
        "        targetcorpus.append(te_e[iter].split())\n",
        "    return testcorpus, targetcorpus\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxSe0n8-Pa9t"
      },
      "source": [
        "testcorpus, targetcorpus = test(model)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjYyYvvy0I20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d3e8f1-04f1-4f9e-eeab-5d663ad64632"
      },
      "source": [
        "score = 0\n",
        "cnt = 0\n",
        "for i in range(len(testcorpus)):\n",
        "    cnt += 1\n",
        "    score += sentence_bleu(testcorpus[i], targetcorpus[i])\n",
        "score /= cnt"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYfs8D4kBsLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e96525d-3782-482a-f367-8413d7c3a1b2"
      },
      "source": [
        "score"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6164856304896983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    }
  ]
}